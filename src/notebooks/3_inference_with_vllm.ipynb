{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95009d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a338e69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_OUTPUT_FILEPATH = \"../data/50_data_points_from_alpaca.csv\"\n",
    "MODEL = \"Qwen/Qwen3-4B\"\n",
    "# MODEL = \"Qwen/Qwen3-4B-AWQ\" # When using quantized model\n",
    "URL = \"http://localhost:8000/v1/chat/completions\"\n",
    "IS_CONCISE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dddab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inference = pd.read_csv(SAMPLE_OUTPUT_FILEPATH)\n",
    "inference_requests = [\n",
    "    {\n",
    "        \"model\": MODEL,\n",
    "        \"stream\": False,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"Please keep the answer short and concise.\"},\n",
    "            {\"role\": \"user\", \"content\": sample.instruction},\n",
    "        ] if IS_CONCISE else [\n",
    "            {\"role\": \"user\", \"content\": sample.instruction},\n",
    "        ],\n",
    "        \"temperature\": 0,\n",
    "    }\n",
    "    for sample in df_inference.itertuples()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3647d54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_requests(\n",
    "    inference_requests: list[dict],\n",
    "    url: str,\n",
    ") -> list[dict]:\n",
    "\n",
    "    def _make_request(request_data: dict) -> dict[str, Any]:\n",
    "        start_time = datetime.datetime.now()\n",
    "        response = requests.post(url, json=request_data)\n",
    "        end_time = datetime.datetime.now()\n",
    "\n",
    "        data = response.json()\n",
    "        data[\"start_time\"] = start_time\n",
    "        data[\"end_time\"] = end_time\n",
    "        return data\n",
    "\n",
    "    results = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(_make_request, req) for req in tqdm(inference_requests)]\n",
    "        results = [future.result() for future in tqdm(as_completed(futures), total=len(futures))]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35f7181",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(inference_requests)\n",
    "results = send_requests(inference_requests, url=URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17225c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_runtime_outliers_by_percentile(results: list[dict]) -> list[dict]:\n",
    "    # Here we simply filter out requests with 5% longest/shortest runtime as outliers\n",
    "    runtimes = [(result[\"end_time\"] - result[\"start_time\"]).total_seconds() for result in results]\n",
    "    lower_bound = pd.Series(runtimes).quantile(0.05)\n",
    "    upper_bound = pd.Series(runtimes).quantile(0.95)\n",
    "    filtered_results = [\n",
    "        result for result in results\n",
    "        if lower_bound <= (result[\"end_time\"] - result[\"start_time\"]).total_seconds() <= upper_bound\n",
    "    ]\n",
    "    return filtered_results\n",
    "\n",
    "filtered_results = trim_runtime_outliers_by_percentile(results)\n",
    "run_time = (max(result[\"end_time\"] for result in filtered_results) - min(result[\"start_time\"] for result in filtered_results)).total_seconds()\n",
    "n_tokens = sum([req[\"usage\"][\"completion_tokens\"] for req in filtered_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b6d00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print throughput metrics\n",
    "print(f\"Total run time: {run_time:.2f} seconds\")\n",
    "print(f\"Total tokens generated: {n_tokens}\")\n",
    "print(f\"Throughput (tokens/sec): {n_tokens / run_time:.2f}\")\n",
    "print(f\"Throughput (requests/sec): {len(results) / run_time:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm-py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
