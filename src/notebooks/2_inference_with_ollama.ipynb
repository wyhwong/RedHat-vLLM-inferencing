{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd3547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaf7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_OUTPUT_FILEPATH = \"../data/50_data_points_from_alpaca.csv\"\n",
    "MODEL = \"qwen3:4b\"\n",
    "URL = \"http://localhost:11434/api/chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ab562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inference = pd.read_csv(SAMPLE_OUTPUT_FILEPATH)\n",
    "inference_requests = [\n",
    "    {\n",
    "        \"model\": MODEL,\n",
    "        \"stream\": False,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": sample.instruction},\n",
    "        ],\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.0,\n",
    "            \"top_k\": 20,  # Added to align with vLLM default settings\n",
    "            \"top_p\": 0.95,  # Added to align with vLLM default settings\n",
    "        },\n",
    "    }\n",
    "    for sample in df_inference.itertuples()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66835ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_requests(\n",
    "    inference_requests: list[dict],\n",
    "    url: str,\n",
    ") -> list[dict]:\n",
    "\n",
    "    def _make_request(request_data: dict) -> dict[str, Any]:\n",
    "        start_time = datetime.datetime.now()\n",
    "        response = requests.post(url, json=request_data)\n",
    "        end_time = datetime.datetime.now()\n",
    "\n",
    "        data = response.json()\n",
    "        data[\"start_time\"] = start_time\n",
    "        data[\"end_time\"] = end_time\n",
    "        return data\n",
    "\n",
    "    results = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(_make_request, req) for req in tqdm(inference_requests)]\n",
    "        results = [future.result() for future in tqdm(as_completed(futures), total=len(futures))]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5642e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(inference_requests)\n",
    "results = send_requests(inference_requests, url=URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb79542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_out_outliers(results: list[dict]) -> list[dict]:\n",
    "    # Here we simply filter out requests with 5% longest/shortest runtime as outliers\n",
    "    runtimes = [(result[\"end_time\"] - result[\"start_time\"]).total_seconds() for result in results]\n",
    "    lower_bound = pd.Series(runtimes).quantile(0.05)\n",
    "    upper_bound = pd.Series(runtimes).quantile(0.95)\n",
    "    filtered_results = [\n",
    "        result\n",
    "        for result in results\n",
    "        if lower_bound <= (result[\"end_time\"] - result[\"start_time\"]).total_seconds() <= upper_bound\n",
    "    ]\n",
    "    return filtered_results\n",
    "\n",
    "\n",
    "filtered_results = filter_out_outliers(results)\n",
    "run_time = (\n",
    "    max(result[\"end_time\"] for result in filtered_results) - min(result[\"start_time\"] for result in filtered_results)\n",
    ").total_seconds()\n",
    "n_tokens = sum([req[\"usage\"][\"completion_tokens\"] for req in filtered_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078cdc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print throughput metrics\n",
    "print(f\"Total run time: {run_time:.2f} seconds\")\n",
    "print(f\"Total tokens generated: {n_tokens}\")\n",
    "print(f\"Throughput (tokens/sec): {n_tokens / run_time:.2f}\")\n",
    "print(f\"Throughput (requests/sec): {len(results) / run_time:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm-py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
